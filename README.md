# mBART_backtranslation_experiment

Back-translation has been proven to be an effective method for improving performance of machine translation models for low-resource languages. In this study, we assess the impact of two variables on the final Swahili to English translation quality of a fine-tuned mBART-50 model: 
1. The quality of the pre-trained model used to generate the back-translations for fine-tuning
2. The amount of back-translated pairs used to fine-tune the baseline mBART-50 model 

We created experimental back-translated datasets using 4 different pre-trained models - GNMT, M2M-100, GPT-3.5, and mBART-50 - and then used the datasets to fine-tune the mBART-50 baseline model. The final Swahili to English translations generated by the fine-tuned models were then evaluated by calculating BLEU scores using the FloRes-200 dataset. 

The findings support our original hypothesis - that applying higher quality pre-trained models to generate the back-translations, and using a larger amount of back-translated data for fine-tuning, will ultimately lead to higher translation quality in the final fine-tuned model.
